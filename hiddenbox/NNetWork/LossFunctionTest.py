import numpy as np
import NNetWork.Variables as uvar # piline : ignore bulid in function
import NNetWork.Layers as nn
from NNetWork.Util import *
from NNetWork.LossFunc import *

import matplotlib.pyplot as plt

def test_sigmoidCrossEntropyWithLogits():
    x = np.linspace(-5,5, num=100).reshape(1,100)
    z = np.array([[5, 4, 2, 9, 6, 7, 2, 1, 8, 4, 4, 0, 8, 3, 5, 9, 1, 8, 3, 4, 9, 7,
        9, 6, 6, 2, 9, 1, 4, 5, 2, 7, 0, 8, 7, 8, 1, 5, 8, 9, 4, 7, 5, 3,
        8, 4, 2, 0, 7, 1, 0, 0, 3, 4, 7, 8, 3, 4, 6, 7, 5, 0, 3, 7, 1, 0,
        2, 4, 4, 1, 5, 1, 9, 8, 0, 4, 6, 5, 0, 7, 6, 7, 9, 4, 1, 6, 4, 0,
        5, 7, 9, 4, 1, 0, 2, 8, 4, 7, 1, 2]])
    X = uvar.BaseVar(x)
    Z = uvar.BaseVar(z)
    loss = sigmoidCrossEntropyWithLogits(X,Z)
    tloss = np.array([[ 2.50067153e+01,  1.96033861e+01,  9.60417217e+00,
         4.22818088e+01,  2.75857995e+01,  3.14757498e+01,
         8.80015510e+00,  4.30650161e+00,  3.35503577e+01,
         1.63802221e+01,  1.59779285e+01,  2.02614198e-02,
         3.03254213e+01,  1.10853478e+01,  1.79566287e+01,
         3.13938342e+01,  3.41719290e+00,  2.62994614e+01,
         9.58612624e+00,  1.23681312e+01,  2.68677365e+01,
         2.02061954e+01,  2.50603201e+01,  1.61271287e+01,
         1.55278853e+01,  5.03032281e+00,  2.14526833e+01,
         2.37078894e+00,  8.79480942e+00,  1.04722927e+01,
         4.06998274e+00,  1.32243249e+01,  1.57626779e-01,
         1.35063413e+01,  1.11493488e+01,  1.19251283e+01,
         1.59135277e+00,  6.56226229e+00,  9.56522861e+00,
         9.84277445e+00,  4.16267332e+00,  6.36340291e+00,
         4.17232558e+00,  2.38750521e+00,  4.89790792e+00,
         2.30966340e+00,  1.23899292e+00,  5.74834590e-01,
         1.68086253e+00,  7.18718517e-01,  7.18718517e-01,
         7.71771621e-01,  6.97840846e-02, -5.28683843e-01,
        -2.23579115e+00, -3.43542541e+00, -8.95323075e-01,
        -1.88828048e+00, -3.93962739e+00, -5.43328627e+00,
        -3.94510434e+00,  1.43391548e+00, -2.27612155e+00,
        -7.95410177e+00,  2.07956571e-01,  1.75540942e+00,
        -1.49365868e+00, -5.14540352e+00, -5.46254383e+00,
         1.30588799e-01, -8.16407097e+00,  1.07940734e-01,
        -1.80837565e+01, -1.65271146e+01,  2.55557533e+00,
        -7.65393291e+00, -1.33173158e+01, -1.10507910e+01,
         2.93346813e+00, -1.78292332e+01, -1.53591415e+01,
        -1.90502374e+01, -2.62257911e+01, -1.01181606e+01,
         3.01978245e-02, -1.79019571e+01, -1.10358644e+01,
         3.81026981e+00, -1.55352941e+01, -2.39210614e+01,
        -3.27106870e+01, -1.25607535e+01,  1.35723163e-02,
         4.40621570e+00, -4.48384612e+00, -3.21616752e+01,
        -1.40818276e+01, -2.87796662e+01,  7.42646410e-03,
        -4.99328465e+00]])
    print("diff of loss", rel_error(loss, tloss))

    print("test backword")

    fx = lambda o: sigmoidCrossEntropyWithLogits(uvar.BaseVar(o),Z)
    ndx = eval_numerical_gradient(fx, x)
    sigmoidCrossEntropyWithLogits(X,Z)
    print("diff of dx", rel_error(X.grad, ndx))

def test_least_square():
    x = np.linspace(-5,5,num=100).reshape(1,100)
    y = np.array([[ 2.,  2.,  8.,  8.,  0.,  6.,  2.,  2.,  2.,  4.,  7.,  0.,
          1.,  0.,  0.,  0.,  5.,  7.,  3.,  1.,  8.,  2.,  1.,  8.,
          7.,  2.,  2.,  4.,  4.,  5.,  0.,  3.,  5.,  9.,  9.,  2.,
          6.,  6.,  8.,  9.,  2.,  8.,  5.,  4.,  3.,  7.,  0.,  8.,
          1.,  1.,  7.,  9.,  4.,  4.,  4.,  6.,  3.,  7.,  6.,  1.,
          4.,  8.,  2.,  2.,  1.,  9.,  4.,  0.,  4.,  9.,  3.,  7.,
          0.,  7.,  8.,  6.,  2.,  4.,  5.,  5.,  4.,  0.,  2.,  0.,
          9.,  7.,  0.,  6.,  5.,  5.,  2.,  3.,  3.,  1.,  5.,  5.,
          1.,  5.,  5.,  8.]])
    tloss = np.array([[ 4.9000e+01,  4.7596e+01,  1.6379e+02,  1.6121e+02,  2.1123e+01,
          1.1014e+02,  4.0882e+01,  3.9601e+01,  3.8340e+01,  6.5463e+01,
          1.2078e+02,  1.5123e+01,  2.2924e+01,  1.3593e+01,  1.2858e+01,
          1.2144e+01,  7.0289e+01,  1.0574e+02,  3.8215e+01,  1.6653e+01,
          1.2056e+02,  2.3803e+01,  1.4272e+01,  1.1399e+02,  9.1695e+01,
          2.0023e+01,  1.9130e+01,  3.9347e+01,  3.8090e+01,  4.9995e+01,
          3.8797e+00,  2.3704e+01,  4.5801e+01,  1.1378e+02,  1.1163e+02,
          1.2004e+01,  5.4223e+01,  5.2746e+01,  8.3935e+01,  1.0122e+02,
          8.7592e+00,  7.8475e+01,  3.3150e+01,  2.1684e+01,  1.2642e+01,
          5.5570e+01,  1.2499e-01,  6.8104e+01,  1.3260e+00,  1.1036e+00,
          4.8295e+01,  7.8296e+01,  1.4044e+01,  1.3297e+01,  1.2570e+01,
          2.9642e+01,  5.4917e+00,  3.8968e+01,  2.6434e+01,  1.6325e-03,
          8.6400e+00,  4.6763e+01,  5.4372e-01,  4.0496e-01,  2.1590e-01,
          5.5269e+01,  5.4444e+00,  3.1247e+00,  4.5425e+00,  4.9425e+01,
          8.6359e-01,  2.3312e+01,  5.1653e+00,  2.1402e+01,  3.0528e+01,
          1.1725e+01,  4.5801e-01,  1.4938e+00,  4.4995e+00,  4.0812e+00,
          8.4491e-01,  1.0124e+01,  1.6456e+00,  1.1450e+01,  3.0417e+01,
          1.1656e+01,  1.3593e+01,  4.8935e+00,  1.2346e+00,  1.0203e+00,
          4.3719e+00,  1.4207e+00,  1.6717e+00,  1.1519e+01,  2.5508e-01,
          1.6325e-01,  1.3668e+01,  4.0812e-02,  1.0203e-02,  9.0000e+00]])

    X,Y = uvar.BaseVar(x), uvar.BaseVar(y)
    loss = leastSquare(X,Y)
    print("loss error", rel_error(loss, tloss))
    
    fx = lambda o: leastSquare(uvar.BaseVar(o), Y)
    ndx = eval_numerical_gradient(fx,x)
    leastSquare(X,Y)
    print("dx error", rel_error(ndx, X.grad))

